---
title: "Impact of novel Alzheimer’s disease drug discovery on the research field using text mining and topic models"
author: "Jess Scrimshire"
date: "`r Sys.Date()`"
engine: knitr
execute:
  echo: false
  include: true
  error: false
  message: false
  warning: false
  cache: true
  freeze: true
toc: true
toc-location: left
bibliography: references.bib
csl: university-of-york-harvard-environment.csl
---

# Abstract

Alzheimer’s disease (AD) is a chronic neurodegenerative disease affecting over 55 million people worldwide. Currently there are no treatments that cure AD, however recently the anti-amyloid immunotherapy lecanemab has been granted accelerated approval by the FDA. Every year thousands of scientific articles are published concerning AD, however it can be time consuming and laborious to disseminate this information into a systematic review whilst highlighting current research. Full abstract text from PubMed for documents containing the MeSH term “Alzheimer’s Disease” were tokenised, cleaned then allocated to either a “pre-leca” or “post-leca” corpus relative to the date of lecanemab accelerated approval, 6th January 2023. Ten-topic latent Diurich Allocation (LDA) models were created for each corpus either using unigrams or bigrams.

```{r}
#| label: packages
#| include: false
library(rmarkdown)
source("scripts/00_setting_up.R")
```

```{r}
#| label: data
#| include: false
load("updated_project.RData")
# abstract_unigrams_clean <- read_csv("results/abstract_unigrams_clean.csv")
# abstract_trigrams <- read_csv("results/abstract_trigrams.csv")
# abstract_bigrams <- read_csv("results/abstract_bigrams.csv")

abstract_n <- abstract_unigrams_clean %>%
  distinct(title) %>% count() %>% pull(n)
```

# Introduction

## Alzheimer's Disease

Alzheimer’s disease (AD) is a chronic neurodegenerative disease and the most common cause of dementia. Affecting over 55 million people worldwide, the predominant symptoms of AD usually manifest after the age of 65 and include cognitive impairment, as well as physical and emotional difficulties [@noauthor_2023-ek]. These symptoms usually manifest after the physiological changes in the brain develop, therefore scientific understanding and advances in new treatments is important in understanding AD aetiology.

Whilst the underlying mechanisms determining the progression of AD are not fully understood, the accumulation of abnormal protein aggregates including, amyloid-beta plaques and neurofibrillary tangles, are frequently found in the brains of patients with AD [@Villemagne2013-xk] [@Iaccarino2018-ta]. This leads to disruptions in neuronal signalling pathways and can result in brain atrophy. Diagnosing AD requires the presence of both amyloid and tau pathologies as well as neuroinflammation, neuronal death and brain atrophy [@Garcia-Morales2021-zb].

## Treatments for AD

Although there are currently no therapies or interventions that can cure AD, several disease-modifying treatments exist that can alter the course of the disease, alleviate symptoms, and enhance the overall quality of life for patients. A recent comprehensive review by [@Huang2023-vq], identified a shift in clinical trial research, which highlighted more Phase I studies being conducted and more Phase III trials involving anti-amyloid therapies. These trials are involving more patients with early onset AD and mild cognitive impairment (MCI) to help develop preventative therapies. Global estimates for people living with preclinical AD or positive for AD pathology biomarkers were 69 and 315 million, respectively [@Gustavsson2023-qr], therefore increasing research focus on these patient populations is imperative to slow the progression of the disease.

There have recently been two drugs granted approval by the United States Food and Drug Administration (US FDA) which target the pathophysiologies of AD; aducanumab and lecanemab [@Center_for_Drug_Evaluation2023-gy]; [@Office_of_the_Commissioner2023-hu]. These treatments are human monoclonal immunotherapies which aim to target and reduce the beta-amyloid protein aggregates in the brain by binding to its various forms in the amyloid-beta pathway. Furthermore, there are four more anti-amyloid monoclonal antibody treatments which have undergone or are currently in Phase III clinical trials [@Cummings2023-lo]. For this study, we will focus on lecanemab, the most recent AD treatment to undergo accelerated approval by the FDA, which was fully approved for treatment in early AD on the 6th July 2023.

## Topic Modelling and Text Mining in AD Research

Given the consistent publication of thousands of articles every year concerning AD, focusing on early-stage drug discovery could lead to more research literature and clinical findings being published. Systematic reviews and meta-analyses are time consuming and labour intensive, and pose a significant challenge to updating the current understandings in the research field [@Higgins2019-kn]. Topic modelling, a prominent text mining technique, can find patterns and relationships within natural language data, and could provide an automated and unbiased overview of research text. The most common topic modelling method is Latent Dirichlet Allocation (LDA) which assumes, for unstructured text data like research publication, that each document is made up of a number of topics and that each topic is made up of a collection of words [@Blei2003-lh]. Each LDA topic is represented as a probability of words within a topic and a probability of topics within each document, which each follow a Dirichlet distribution.

In silico topic modelling has been used for various applications relating to AD, including describing the research landscape. [@Martinelli2022-ic], identifying novel biomarkers [@Greco2012-pv], and drug repurposing [@Nian2022-xw]. Martinelli performed a nine-topic LDA model and identified five mechanistic themes, one topic relating to AD diagnosis and three concerning treatments. To the best of my knowledge, no studies have explored the change to the AD research landscape with the emergence of newly approved immunotherapy treatments. To increase the findability and reduce bias when selecting articles, the litsearchr package will help identify the most important terms which we will reference our search terms against [@Grames2019-as].

## Aims and Hypotheses

In this study we aim to comprehensively investigate current research in AD, focusing on the period around the accelerated approval of the AD immunotherapy drug, lecanemab. We hypothesised that major topics in AD research could be categorised into distinct themes by applying LDA topic models to existing literature. Additionally, we hope to determine whether these topics have changed with the emergence of new immunotherapy treatments. This method is justified for the vast quantity of literature being published concerning AD and could be translated into other disease areas to study the impact of novel treatment options. Remaining current with the latest research is crucial for making significant strides in understanding and addressing the complexities of this disease, and finding new ways to treat AD.

# Methods

A full summary of the methodology is provided in Appendix 1. All data analysis and visualisations were done in R version 4.3.2 using *tidyverse* packages [@Wickham2019-rj] unless otherwise stated.

## Data Acquisition

Due to accessing constraints, abstracts represent the only document content for this study. Titles, full abstract text, and publication date were obtained from the National Center for Biotechnology Information (NCBI) datasbase, PubMed, using the inclusion criteria described in @tbl-inclusion-criteria, and accessed through *Rismed* [@Kovalchik2021-xq] on 18th February 2023. The diversity of entries into the PubMed database ensures that the contents are representative and studies are reliable as they are obtained from multiple sources. Entries are assigned Medical Subject Headings (MeSH) which identify health-related terms within each document, therefore classifying articles according to their subject nature which reduces potential interpretive bias.

+----------------------------+------------------------------------------------+
| Criteria                   | Filter Applied                                 |
+============================+================================================+
| MeSH Term                  | 'Alzheimer's Disease'                          |
+----------------------------+------------------------------------------------+
| Title and/or Abstract Text | 'Alzheimer's Disease'                          |
|                            |                                                |
|                            | 'alzheimer'                                    |
|                            |                                                |
|                            | 'AD'                                           |
+----------------------------+------------------------------------------------+
| Article Type               | "Books"                                        |
|                            |                                                |
|                            | "Case Reports"                                 |
|                            |                                                |
|                            | "Clinical Study"                               |
|                            |                                                |
|                            | "Clinical Trial"                               |
|                            |                                                |
|                            | "Controlled Clinical Trial"                    |
|                            |                                                |
|                            | "Meta-analysis"                                |
|                            |                                                |
|                            | "Randomised Controlled Trial"                  |
|                            |                                                |
|                            | "Review"                                       |
|                            |                                                |
|                            | "Systematic Review"                            |
+----------------------------+------------------------------------------------+
| Publication Date           | 1st January 2022 to 1st January 2024 inclusive |
+----------------------------+------------------------------------------------+
| Language                   | English                                        |
+----------------------------+------------------------------------------------+

: Inclusion criteria used to query and identify all relevant terms concerning AD in the PubMed database. Adapted from [@Martinelli2022-ic]. {#tbl-inclusion-criteria}

### *litsearchR*

To reassure us that the PubMed search query encapsulated all literature, *litsearchr* package was used to expand the search terms [@Grames2019-as]. Citations from PubMed results using the previous search criteria in @tbl-inclusion-criteria were read into R. The combined unique keyword and titles, as not all articles have keywords, for each result were collected. To ensure only the most relevant terms were searched, stop words were removed, as previously described, and the minimum frequency of words was set as n = 50 for keywords and n = 75 for the title. A matrix of each word in each article was created and the potential search terms were ranked with *create_network* and *strength* [@Barrat2004-hm] from the *igraph* package [@Csardi2006-gb].

```{r}
#| label: search-terms
#| include: false
#| cache: true

naive_results <- import_results(file="data/pubmed-alzheimerd-set.nbib")
naive_drug_results <- import_results(file="data/pubmed-lecanemabO-set.nbib")

nrow(naive_results)

keywords <- extract_terms(keywords=naive_results[, "keywords"], 
                          method="tagged", 
                          min_n = 1, # allows single words
                          min_freq = 50) # only words that appear at least 10 times in keyword search 

# Remove stop-words from titles
clin_stopwords <- read_lines("data/clin_stopwords.txt")
all_stopwords <- c(get_stopwords("English"), clin_stopwords)

title_terms <- extract_terms(
  text = naive_results[, "title"],
  method = "fakerake",
  min_freq = 75, 
  min_n = 1,
  stopwords = all_stopwords
)

search_terms <- c(keywords, title_terms) %>% unique()


### Network analysis ###

# Combine title with abstract
docs <- paste(naive_results[, "title"], naive_results[, "abstract"])

# Create matrix of which term appears in which article
dfm <- create_dfm(elements = docs, 
                  features = search_terms)

# Create network of linked terms
g <- create_network(dfm, 
                    min_studies = 3)
ggraph(g, layout="stress") +
  coord_fixed() +
  expand_limits(x=c(-3, 3)) +
  geom_edge_link(aes(alpha=weight)) +
  geom_node_point(shape="circle filled", fill="white") +
  geom_node_text(aes(label=name), 
                 hjust="outward", 
                 check_overlap=TRUE) 

## Pruning ##

# Remove terms that are not connected to other terms - strength
strengths <- strength(g)

term_strengths <- data.frame(term=names(strengths), strength=strengths, row.names=NULL) %>%
  mutate(rank = rank(strength, 
                   ties.method="min")) %>%
  arrange(strength)

# Visualise to determine cutoff
cutoff_fig <- ggplot(term_strengths, aes(x=rank, 
                                         y=strength, 
                                         label=term)) +
  geom_line() +
  geom_point() +
  geom_text(data=filter(term_strengths, rank>5), hjust="right", nudge_y=20, check_overlap=TRUE)

cutoff_fig

# Find 80% cutoff
cutoff_cum <- find_cutoff(g, 
                          method="cumulative", 
                          percent=0.8)

# Add to figure
cutoff_fig +
  geom_hline(yintercept = cutoff_cum, 
             linetype = "dashed")

# Add cutoffs for changes
cutoff_change <- find_cutoff(g, 
                             method = "changepoint", 
                             knot_num = 3)

```

## Data Preprocessing

Abstracts and their metadata were categorised into *“pre-leca”* or *“post-leca”* corpuses based on their publication date relative to the date of lecanemab’s accelerated early approval, 6th January 2023 [@Office_of_the_Commissioner2023-hu]. Full abstract text was tokenised into single words using the *unnest_tokens* function of the *tidytext* package [@Silge2016-jf]. The same function was used for tokenising to bigrams and trigrams, using n = 2 and n = 3 respectively. Stop words from the *tidytext* package [@Silge2016-jf] combined with the personalised words frequent to the unigram analysis, "*alzheimer's*", "*ad*", "*95*", "*ci*", and "*including*", were then removed. To prevent the different spellings of the same phrase from being counted multiple times, similar bigrams and trigrams were mapped to the same variable. For example, ‘*amyloid β*, ‘*beta amyloid*’, and ‘*amyloid aβ*’ were all mapped to ‘*amyloid beta*’, and '*mild cognitive impairments*' and '*cognitive impairment mci*' were mapped to ‘*mild cognitive impairment*’.

```{r}
#| label: fig-preprocess
#| include: true
#| fig-cap: "**Abstract tokenisation into unigrams.** Schematic of an abstract being (A) tokenised into single-word tokens followed by (B) removal of stop words obtained from the *tidytext* package [@Silge2016-jf] and personalised words frequent to the unigram analysis, alzheimers, AD, 95, ci, and including. Tokenisation and data cleaning of bigrams and trigrams followed the same methods, not shown."
#| fig-subcap:
#|  - "Example of Tokenisation"
#|  - "Removal of stopwords"
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

knitr::include_graphics("data/preprocess1.png")
knitr::include_graphics("data/preprocess2.png")
```

```{r}
#| label: bigram-cleaning
#| include: false
bigrams_separated <- abstract_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
bigrams_separated <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word &
         !word1 %in% my_stopwords$word) %>% # remove word1 if stopword
  filter(!word2 %in% stop_words$word &
         !word2 %in% my_stopwords$word) # remove word2 if stopword

# join word1 and word2 back together into bigram
bigrams_united <- bigrams_separated %>%
  unite(bigram, word1, word2, sep = " ")  # 'bigram' name of new column
  
## Map words and remove abbreviations ##
bigrams_united <- bigrams_united %>% 
 # filter(grepl("alzheimer*\\b*disease*", bigram)) %>% 
  mutate(bigram = str_replace_all(bigram, 
                              "\\b(neurodegenerative dis(?:ease|eases|order|orders)?)\\b|\\b(neurological dis(?:ease|eases|order|orders)?)\\b", 
                              "neurodegenerative disease"),
         bigram = str_replace_all(bigram,
                                  "\\b(central nervous)\\b|\\b(system cns)\\b",
                                  "cns"),
         bigram = str_replace_all(bigram,
                                  "\\b(parkinson's disease)\\b|\\b(disease pd)\\b|\\b(parkinson's diseases)\\b|\\b(parkinson's pd)\\b|\\b(parkinson disease)\\b|\\b(disease parkinson's)\\b",
                                  "parkinson's disease"),
         bigram = str_replace_all(bigram,
                                  "\\b(huntington's disease)\\b|\\b(disease hd)\\b|\\b(huntington's diseases)\\b|\\b(huntingon's hd)\\b|\\b(huntington disease)\\b|\\b(disease huntington's)\\b",
                                  "huntington's disease"),
         bigram = str_replace_all(bigram,
                                  "\\b(blood brain)\\b|\\b(brain barrier)\\b",
                                  "blood brain"),
         bigram = str_replace_all(bigram,
                                  "\\b(amyloid\\s*\\p{Greek})\\b|\\b(amyloid beta)\\b|\\b(beta amyloid)\\b|\\b(β aβ)\\b|\\b(amyloid β)\\b|\\b(beta aβ)\\b",
                                   "amyloid beta"),
         bigram = str_replace_all(bigram,
                                  "\\b(2019 covid)\\b|\\b(covid 19)\\b|\\b(sars cov)\\b|\\b(cov 2)\\b",
                                  "covid 19"),
         bigram = str_replace_all(bigram,
                                  "\\b(2 diabetes)\\b|\\b(diabetes t2d)\\b|\\b(type 2)\\b",
                                  "diabetes t2d"),
         bigram = str_replace_all(bigram,
                                  "\\b(amyotrophic lateral)\\b|\\b(lateral sclerosis)\\b|\\b(disease amyotrophic)\\b",
                                  "als"),
         bigram = str_replace_all(bigram,
                                   "\\b(α syn|α synuclein|alpha synuclein|alpha synucleinuclein)\\b",
                                  "alpha synuclein"),
         bigram = str_replace_all(bigram,
                                  "\\b(cerebrospinal fluid)\\b|\\b(fluid csf)\\b|\\b(cerebrospinal csf)\\b",
                                  "cerebrospinal fluid"),
         bigram = str_replace_all(bigram,
                                  "\\b(cardiovascular dis(?:ease|eases|order|orders)?)\\b",
                                  "cardiovascular disease"))

# All abstracts
bigram_counts <- bigrams_united %>% 
  group_by(type) %>% 
  count(bigram, sort = TRUE) %>% 
  ungroup()

```

```{r}
#| label: trigram clean
#| include: false
trigrams_separated <- abstract_trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")
trigrams_separated <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word &
           !word1 %in% my_stopwords$word) %>% # remove word1 if stopword
  filter(!word2 %in% stop_words$word &
           !word2 %in% my_stopwords$word) %>% # remove word2 if stopword
  filter(!word3 %in% stop_words$word &
           !word3 %in% my_stopwords$word) # remove word3 if stopword

trigrams_united <- trigrams_separated %>%
  group_by(type) %>% 
  unite(trigram, word1, word2, word3, sep = " ") %>%  # 'trigram' name of new column
  ungroup()

trigrams_united <- trigrams_united %>%
  mutate(trigram = str_replace_all(trigram, 
                                   "\\b(?:mild cognitive impairment|cognitive impairment mci)\\b", 
                                   "mild cognitive impairment"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:central nervous system|nervous system cns|central nervous system)\\b", 
                                   "central nervous system"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:blood brain barrier|brain barrier bbb)\\b", 
                                   "blood-brain barrier"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:amyotrophic lateral sclerosis|lateral sclerosis als|amyotropic lateral sclerosis|disease amyotrophic lateral)\\b",
                                   "amyotrophic lateral sclerosis"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:amyloid beta aβ|β amyloid aβ|amyloid β aβ|amyloid β peptide|amyloid β protein)\\b",
                                   "amyloid beta aβ"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:type 2 diabetes|diabetes mellitus t2dm|2 diabetes mellitus)\\b", 
                                   "type 2 diabetes"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:huntington's disease hd|disease huntington's disease|disease huntington disease|huntington disease hd|huntington's diseases hd)\\b",
                                   "huntington's disease hd"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:parkinson's disease pd|disease parkinson's disease|disease parkinson disease|parkinson disease pd|parkinson's diseases pd)\\b",
                                   "parkinson's disease pd"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:multiple sclerosis ms|disease multiple sclerosis)\\b",
                                   "multiple sclerosis ms"),
         trigram = str_replace_all(trigram,
                                   "\\b(amyloid precursor protein|precursor protein app)\\b",
                                   "amyloid precursor protein"),
         trigram = str_replace_all(trigram,
                                   "\\b(reactive oxygen species|oxygen species ros)\\b",
                                   "reactive oxygen species"),
         trigram = str_replace_all(trigram,
                                   "\\b(magnetic resonance imaging|resonance imaging mri)\\b",
                                   "magnetic resonance imaging"),
         trigram = str_replace_all(trigram,
                                   "\\b(randomized controlled trial|randomized controlled trials|randomized clinical trials|randomized clinical trial|randomised controlled trials|randomised controlled trial|controlled trials rcts|controlled trial rct)\\b",
                                   "randomised controlled trials"),
         trigram = str_replace_all(trigram,
                                   "\\b(lewy body dementia|lewy bodies dlb|dementia lewy body)\\b",
                                   "lewy body dementia"))

trigram_counts <- trigrams_united %>% 
  group_by(type) %>%
  count(trigram, sort = TRUE) %>%
  ungroup()
```

## Data Analysis

### Publication Frequency

Number of abstracts published per month were visualised as well as the frequency of dates of publications for papers containing the associated terminology for the AD drug lecanemab were also obtained.

### Term Frequency

#### N-gram Frequency Analysis

After tokenisation, the top 20 most frequent unigrams were determined for each dataset. The top 20 most frequent bigrams and trigrams were also determined due to many unigrams being associated with pairs or triplets of words. For example, “mild cognitive impairment” relates to a neurological condition, whereas the words “mild”, “cognitive” and “impairment” have ambiguous connotations individually.

#### Term Usage Over Time

The distribution of terms used over 1000 times from the unigram analysis was visualised. Generalised linear model (GLM) was used to determine whether there was a significant change in word usage over the months.

```{r}
#| label: fig-glm-words
#| include: false

## Create Generalised linear model

top_words <- c("disease", "brain", "studies", "diseases", "review", "cognitive", "dementia", "neurodegenerative", "clinical", "patients", "treatment", "tau", "amyloid")

# Get word frequency per month
glm_abstracts <- abstract_unigrams_clean %>%  
  filter(word %in% top_words) %>% 
  mutate(date = floor_date(date, "month")) %>% # round date to month
  group_by(date) %>%
  count(word, sort = TRUE) %>%
  ungroup() %>%
  group_by(word) %>%
  mutate(freq = n / sum(n)) %>%
  ungroup() 

# Generalised linear model
glm <- glm( freq ~ date + word, data = glm_abstracts, family = "poisson" )
summary(glm)
```

### Topic Modelling

A document term matrix (dtm) was constructed for each dataset, indicating each word’s term frequency (tf), which is a measure of how often a word appears in a document. To determine if a statistical model could distinguish between pre-leca and post-leca corpuses, a two-topic Latent Dirich Allocation (LDA) model [@Blei2003-lh] was applied to the dtm using the *topicmodels* package [@Grun2011-do]. The per-document-per-topic probabilities (γ) was extracted to show the proportion of words generated in each topic and how often these words appear in either the pre-leca or post-leca corpuses.

Two ten-topic LDA models were also created, one for each of the pre-leca and post-leca text corpuses, to determine the most frequent topics, where an arbitrary topic number (k) of ten was chosen. The per-topic-per-word probabilities (β) were extracted and the top 10 terms most commonly found in each topic were visualised. In each model the abstracts are considered mixtures of topics and each topic is considered a mixture of words. As a lot of the most common words may appear as bigrams or trigrams in the text corpuses, the per-topic-per-bigram and per-topic-per-trigram probabilities with the top 10 most common bigrams and trigrams were also visualised with two ten-topic LDA models for the pre-leca and post-leca text corpuses (@fig-topic-models-bigrams ; @fig-topic-models-trigrams).

```{r}
#| label: Gamma LDA Topic Modelling
#| include: false
#| cache: true

# # Create document term matrix of bigrams
# bigram_dtm <- bigrams_separated %>%
#   unite(bigram, word1, word2, sep = " ") %>% 
#   count(abstract, bigram) %>% 
#   cast_dtm(abstract, bigram, n)

unigram_dtm <- abstract_unigrams_clean %>%
  count(abstract, word) %>% 
  cast_dtm(abstract, word, n)

# Create 2 topic LDA model for all abstracts
all_lda <- LDA(unigram_dtm, k = 2, control = list(seed = 1234))

# Get gamma scores
tidy_lda_gamma <- tidy(all_lda, 
                 matrix = "gamma")

# Join results with abstract to get the date variable
abstract_gamm <- tidy_lda_gamma %>%
  mutate(abstract = as.numeric(document)) %>% 
  left_join(abstract_unigrams_clean, by = "abstract") %>% 
  select(document, gamma, topic, date) %>% 
  mutate(type = case_when(date <= leca_approv ~ "pre-leca",
                          date > leca_approv ~ "post-leca"))

## See if significant difference
wilcox.test(abstract_gamm$gamma)

# Wilcoxon signed rank test with continuity correction
# 
# data:  abstract_gamm$gamma
# V = 2.7123e+11, p-value < 2.2e-16
# alternative hypothesis: true location is not equal to 0
```

```{r}
#| label: LDA Topic Model Bigrams
#| include: false
#| cache: true

# Cast the bigram counts into a document term matrix
bigram_dtm_pre <- bigrams_united %>%
  filter(type == "pre-leca") %>% 
  count(abstract, bigram) %>% 
  cast_dtm(abstract, bigram, n) 

bigram_dtm_post <- bigrams_united %>%
  filter(type == "post-leca") %>% 
  count(abstract, bigram) %>% 
  cast_dtm(abstract, bigram, n)

bigram_lda_pre <- LDA(bigram_dtm_pre, k = 10, control = list(seed = 1234))
# A LDA_VEM topic model with 10 topics
bigram_lda_post <- LDA(bigram_dtm_post, k = 10, control = list(seed = 1234))
# A LDA_VEM topic model with 10 topics.

# Interpret the model
tidy_bigram_lda_pre <- tidy(bigram_lda_pre, 
                            matrix = "beta")

tidy_bigram_lda_post <- tidy(bigram_lda_post,
                             matrix = "beta")

# Top 10 terms per topic
#   Not including 'neurodegenerative diseases', parkinson\'s disease' and 'cognitive impairment' as these were common to all bar one topics

top_bigram_terms_pre <- tidy_bigram_lda_pre %>%
  filter(!term %in% c("neurodegenerative disease", "parkinson\'s disease", "amyloid beta")) %>%
  # mutate(topic = case_when(topic == 1 ~ "Mechanisms",
  #                          topic == 2 ~ "Protein",
  #                          topic == 3 ~ "Something else1",
  #                          topic == 4 ~ "Something else2",
  #                          topic == 5 ~ "Something else3",
  #                          topic == 6 ~ "Something else4",
  #                          topic == 7 ~ "Something else5",
  #                          topic == 8 ~ "Something else6",
  #                          topic == 9 ~ "Something other",
  #                          topic == 10 ~ "Something else7")) %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_bigram_terms_post <- tidy_bigram_lda_post %>%
  filter(!term %in% c("neurodegenerative disease", "parkinson\'s disease")) %>%
  # mutate(topic = case_when(topic == 1 ~ "Mechanisms",
  #                          topic == 2 ~ "Protein",
  #                          topic == 3 ~ "Something else1",
  #                          topic == 4 ~ "Something else2",
  #                          topic == 5 ~ "Something else3",
  #                          topic == 6 ~ "Something else4",
  #                          topic == 7 ~ "Something else5",
  #                          topic == 8 ~ "Something else6",
  #                          topic == 9 ~ "Something other",
  #                          topic == 10 ~ "Something else7")) %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualise
top_bigram_terms_pre <- top_bigram_terms_pre %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 bigrams in each LDA topic: Pre-leca",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")


top_bigram_terms_post <- top_bigram_terms_post %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 bigrams in each LDA topic: Post-leca",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

```{r}
#| label: LDA Topic Model Trigrams
#| include: false
#| cache: true

# Cast the bigram counts into a document term matrix
trigram_dtm_pre <- trigrams_united %>%
  filter(type == "pre-leca") %>% 
  count(abstract, trigram) %>% 
  cast_dtm(abstract, trigram, n) 

trigram_dtm_post <- trigrams_united %>%
  filter(type == "post-leca") %>% 
  count(abstract, trigram) %>% 
  cast_dtm(abstract, trigram, n)

trigram_lda_pre <- LDA(trigram_dtm_pre, k = 10, control = list(seed = 1234))
# A LDA_VEM topic model with 10 topics
trigram_lda_post <- LDA(trigram_dtm_post, k = 10, control = list(seed = 1234))
# A LDA_VEM topic model with 10 topics.

# Interpret the model
tidy_trigram_lda_pre <- tidy(trigram_lda_pre, 
                            matrix = "beta")

tidy_trigram_lda_post <- tidy(trigram_lda_post,
                             matrix = "beta")

# Top 10 terms per topic
#   Not including 'neurodegenerative diseases', parkinson\'s disease' and 'cognitive impairment' as these were common to all bar one topics

top_trigram_terms_pre <- tidy_trigram_lda_pre %>%
  filter(!term %in% c("central nervous system", "parkinson's disease pd", "blood-brain barrier")) %>%
    # mutate(topic = case_when(topic == 1 ~ "Mechanisms",
    #                        topic == 2 ~ "Protein",
    #                        topic == 3 ~ "Something else1",
    #                        topic == 4 ~ "Something else2",
    #                        topic == 5 ~ "Something else3",
    #                        topic == 6 ~ "Something else4",
    #                        topic == 7 ~ "Something else5",
    #                        topic == 8 ~ "Something else6",
    #                        topic == 9 ~ "Something other",
    #                        topic == 10 ~ "Something else7")) %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_trigram_terms_post <- tidy_trigram_lda_post %>% 
  filter(!term %in% c("central nervous system", "parkinson's disease pd")) %>%
  # mutate(topic = case_when(topic == 1 ~ "Mechanisms",
  #                          topic == 2 ~ "Protein",
  #                          topic == 3 ~ "Something else1",
  #                          topic == 4 ~ "Something else2",
  #                          topic == 5 ~ "Something else3",
  #                          topic == 6 ~ "Something else4",
  #                          topic == 7 ~ "Something else5",
  #                          topic == 8 ~ "Something else6",
  #                          topic == 9 ~ "Something other",
  #                          topic == 10 ~ "Something else7")) %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)


# Visualise
top_trigram_terms_pre <- top_trigram_terms_pre %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  theme(axis.text.y = element_text(size = 8)) +
  labs(title = "Top 10 trigrams in each LDA topic: Pre-leca",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")


top_trigram_terms_post <- top_trigram_terms_post %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  theme(axis.text.y = element_text(size = 8)) +
  labs(title = "Top 10 trigrams in each LDA topic: Post-leca",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

# Results and Discussion

## Search Query Refinement Identified the Term 'alzheimer'

Out initial search query was refined using *litsearchr* [@Grames2019-as] using the 80% cutoffs which determined the most important terms to the articles; all others were discarded (@fig-ad-search-terms). We disregarded '*alzheimer's disease*' as this MeSH term was included in the original search query, but we updated the PubMed search query with '*alzheimer*' (@tbl-inclusion-criteria). We omitted the unigram '*disease*' as it was too broad and may have encapsulated articles concerning other irrelevant neurodegenerative diseases into our query.

```{r}
#| label: fig-ad-search-terms
#| include: true
#| fig-cap: "** Search query refinement using *litsearchr* identified '*alzheimer*'.** A matrix of each word in each article was created and the potential search terms were ranked with *create_network* and *strength* [@Barrat2004-hm] from the *igraph* package [@Csardi2006-gb]. The top keywords and words from the titles of AD papers, with minimum frequencies of n = 50 and n = 75, respectively, were ranked by their importance to article content. The dashed line marks 80% cutoff, and all terms below were discarded."
#| fig-width: 10
#| fig-height: 7
#| fig-align: left

cutoff_fig +
  geom_hline(yintercept = cutoff_change, 
             linetype="dashed") +
  theme_classic()

```

## AD Research Publication are Similar Over Time

This study found `r abstract_n` papers that were published between `r min(abstract_unigrams_clean$date)` and `r max(abstract_unigrams_clean$date)` that met the inclusion criteria in @tbl-inclusion-criteria. The distribution of publication frequency did not vary considerably during this period @fig-publication-date. The frequency of publications containing the terms associated with the AD drug lecanemab increased in 2023. This could be attributed to 2023 being the year of the accelerated and traditional approval of lecanemab [@Office_of_the_Commissioner2023-hu; @Office_of_the_Commissioner2023-ka]. The overall distribution of dates of publication was similar across the observed time frame for the full abstract texts.

```{r}
#| label: fig-publication-date
#| include: true
#| fig-cap: "**Frequency of lecanemab publications increases in 2023.** Distribution of articles published over time containing (A) the MeSH term ‘Alzheimer’s Disease’ in the title and/or abstract (n = 6,167) or (B) terms associated with the AD drug lecanemab: ‘lecanemab’, ‘leqembi’, ‘BAN2401’, and ‘mAb158’ (n = 207). Line represents date of lecanemab accelerated approval."
#| fig-subcap: 
#|  - "All Abstracts"
#|  - "Lecanemab Abstracts"
#| fig-width: 15
#| fig-height: 10
#| fig-align: left

abstract_unigrams_clean %>%  
  ggplot(aes(date)) +
  geom_histogram(bins = 100) +
  xlab("Date of Publication") +
  ylab("Number of Abstracts") +
  scale_x_date(date_breaks = "4 month", 
               date_labels = "%m/%Y") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, 
                                   vjust = 0.5, 
                                   hjust = 0.5,
                                   size = 15),
        axis.title.x = element_text(size = 25),
        axis.title.y = element_text(size = 25),
        strip.text = element_text(size = 20),
        axis.text.y = element_text(size = 15))  +
  geom_vline(xintercept = as.numeric(as.Date("2023-01-06")), 
             linetype = "dashed", 
             color = "red", 
             linewidth = 1)

## Visualise abstracts published with all lena terms
naive_drug_results %>% 
  filter(!is.na(date_published)) %>% 
  ggplot(aes(as.Date(date_published, format = "%Y %b %d"))) +
  geom_histogram(bins = 30,
                 binwidth = 100) +
  xlab("Date of Publication") +
  ylab("Number of Abstracts") +
  scale_x_date(date_breaks = "6 months", date_labels = "%m/%Y") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, 
                                   vjust = 0.5, 
                                   hjust = 0.5,
                                   size = 15),
        axis.title.x = element_text(size = 25),
        axis.title.y = element_text(size = 25),
        strip.text = element_text(size = 20),
        axis.text.y = element_text(size = 15)) +
  geom_vline(xintercept = as.numeric(as.Date("2023-01-06")), 
             linetype = "dashed", 
             color = "red", 
             linewidth = 1)
```

The per-document-per-topic probabilities (γ) did not show a difference between the two topics when pre-leca and post-leca documents were identified (@fig-gamma-lda).

```{r}
#| label: fig-gamma-lda
#| include: true 
#| fig-cap: "**Two topic LDA model does not distinguish between pre-leca and post-leca corpuses*.* A document term matrix (dtm) was constructed for the unigram dataset and a two topic LDA model was applied to the dtm using *topicmodels* [@Grun2011-do]. Boxplots show first and third quartiles, median and outlier per-document-per-topic probabilities, γ. Topics are split for pre-leca and post-leca corpuses. n = 7,007."
#| fig-width: 15
#| fig-height: 10
#| fig-align: left

abstract_gamm %>% 
  distinct(document, gamma, topic, date, type) %>%
  ggplot() +
  geom_boxplot(aes(x = type, y = gamma),
               alpha = 3)  +
  facet_wrap(~ topic,
             strip.position = "bottom") +
  labs(y = expression(gamma)) +
  xlab("Topic") +
  theme(axis.title.x = element_text(size = 30),
        axis.title.y = element_text(size = 30),
        strip.text = element_text(size = 20),
        axis.text.x = element_text(size = 15),
        axis.text.y = element_text(size = 15),
        legend.position = "none",
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        strip.background = element_blank()) 
```

## N-gram Analysis

Despite the full abstract dataset not significantly splitting into two distinct topics, we aimed to suggest this was because the language was very similar between the two corpses. We therefore aimed to explore the most common n-grams frequencies. The 15 most frequent unigrams, bigrams and trigrams along with the distribution by month of the top 14 shared most frequent unigrams for both corpuses are shown in @fig-tokenisation-1. '*Disease*' is the most frequent unigram in both corpuses. This may be due to being most frequently associated with '*neurodegenerative disease*', '*Parkinson's disease*', and '*Huntington's disease*' which are among the most frequent bigrams. which are common neurodegenerative diseases which are researched as much as AD (ref).

```{r}
#| label: fig-tokenisation
#| include: true
#| fig-cap: "**Literature use is similar around the time of lecanemab approval.** Tokenised n-grams split into pre- and post-lecanemab corpuses and ranked by (A) unigram frequency, (C) bigram frequency, and (D) trigram frequency, removing stop-words (n = 7,007). **Word frequency trends are similar for the top 10 most frequent terms.** The focus of AD research has stayed consistent over time. The frequency of a selection of words used over 1000 times in AD abstracts from 01-01-2022 to 10-10-2023 has not changed."
#| fig-subcap: 
#|  - "Unigram"
#|  - "Most common words"
#|  - "Bigram"
#|  - "Trigram"
#| fig-width: 15
#| fig-height: 10
#| fig-align: left

abstract_unigrams_clean %>% 
  group_by(type) %>%
  count(word, sort = TRUE) %>%
  slice_head(n = 15) %>% 
  ggplot(aes(n, reorder(word, n), fill = type)) +
  geom_col() +
  labs(y = NULL) +
  xlab("Count") +
  theme(axis.title.x = element_text(size = 30),
        axis.title.y = element_text(size = 30),
        strip.text = element_text(size = 20),
        axis.text.x = element_text(size = 15),
        axis.text.y = element_text(size = 15)) +
  facet_wrap(~factor(type, levels=c('pre-leca','post-leca')), 
             scale = "free") +
  theme(legend.position = "none",
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        strip.background = element_blank()) +
  scale_fill_manual(values = c("black", "darkgrey"))

glm %>% ggplot(aes(x = date, y = freq)) +
  geom_line(aes(color = word)) +
  ylab("Frequency") +
  xlab("Date") +
  scale_x_date(date_breaks = "6 months", date_labels = "%m/%Y") +
  theme(axis.title.x = element_text(size = 30),
        axis.title.y = element_text(size = 30),
        strip.text = element_text(size = 20),
        axis.text.x = element_text(size = 15),
        axis.text.y = element_text(size = 15)) +
  theme_classic() +
  scale_color_manual(values = c("#56B4E9", "darkblue", "red", "darkgreen", "orange", "black", "purple", "brown", "pink", "grey", "lightblue", "lightgreen", "lightpink", "lightgrey"))

bigram_counts %>% 
  group_by(type) %>%
  slice_head(n = 15) %>% 
  ggplot(aes(n, reorder(bigram, n), fill = type)) +
  geom_col() +
  labs(y = NULL) +
  xlab("Count") +
  theme(axis.title.x = element_text(size = 30),
        axis.title.y = element_text(size = 30),
        strip.text = element_text(size = 20),
        axis.text.x = element_text(size = 15),
        axis.text.y = element_text(size = 15)) +
  facet_wrap(~factor(type, levels = c("pre-leca", "post-leca")),
             scale = "free") +
  theme(legend.position = "none",
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        strip.background = element_blank()) +
  scale_fill_manual(values = c("darkgrey", "black")) 

trigram_counts %>%
  group_by(type) %>%
  slice_head(n = 15) %>%
  ggplot(aes(x = n, y = reorder(trigram, n), fill = type)) +
  geom_col() +
  labs(y = NULL) +
  xlab("Count") +
  theme(axis.title.x = element_text(size = 30),
        axis.title.y = element_text(size = 30),
        strip.text = element_text(size = 20),
        axis.text.x = element_text(size = 15),
        axis.text.y = element_text(size = 15)) +
  facet_wrap(~factor(type, levels = c("pre-leca", "post-leca")),
             scale = "free") +
  theme(legend.position = "none",
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        strip.background = element_blank()) +
  scale_fill_manual(values = c("darkgrey", "black")) 
```

## Topic Modelling

### Potential increase in late-phase studies after lecanemab approval.

The top 10 unigrams in LDA for **pre-leca:** and **post-leca** are shown in @fig-topic-model-unigrams. Topics that occur in both text corpuses are *'Treatments'*, *'Abnormal Proteins'*, *'Neurodegeneration review'*, *'Risk Factor*s', *'Study Terminology'*, *'Diagnosis'*, and *'Cellular Pathology'*. Within 'abnormal proteins'

Topics concerning study terminology was common to both corpuses, however in the post-leca corpus the unigrams 'randomised', 'trial' and 'trials' were unique and potentially suggests the increase in randomised clinical trials after lecanemab approval. This is consistent with the trends observed in [@Huang2023-vq] which suggest an increase in Phase III clinical trials for anti-amyloid therapies and encompasses the traditonal approval date of lecanemab by the FDA in July 2023 from the Clarity AD clinical trial [@Office_of_the_Commissioner2023-ka]. Randomisation of clinical trials usually happens during the later Phase III trials due to earlier phases recruiting smaller numbers of participants. Therefore, this suggests a shift in the research literature to studies with larger patient cohorts and using robust methods to eliminate selection bias.


```{r}
#| label: fig-topic-model-unigrams 
#| include: true 
#| fig-cap: "**Unigram LDA Topic Modelling.** Outputs from the LDA model with ten topics. Figures show the top ten most commonly associated unigrams in each topic ordered by their per-topic-per-word probability, β. Topic titles were manually created and added." 
#| fig-subcap: 
#|   - "Top 10 terms in each LDA topic: Pre-Leca" 
#|   - "Top 10 terms in each LDA topic: Post-Leca" 
#| fig-width: 20
#| fig-height: 15 
#| fig-align: center

knitr::include_graphics("plots/pre-leca-lda-new.png")
knitr::include_graphics("plots/post-leca-lda-new.png")
```

## Study Limitations

Due to our search strategy, a lot of papers contain the MeSH term 'Alzheimer's disease' may have been mentioned as a collective with other neurodegenerative diseases, hence why the unigrams '*huntington’s disease*', '*parkinson’s disease*' and '*amylotropic lateral sclerosis*' are very frequent. We tried to avoid this by filtering the titles and abstracts to also contain the term 'alzheimer's disease', 'ad' or 'alzheimer'.

One reason for there not being any statistical significance between the per-document-per-topic probabilities could be due to the time frame used for the analysis not being long enough to account for a change in the research landscape. Previous studies have used a five-year time period to detect topics in AD literature[@Martinelli2022-ic], therefore research topics may not be changing in our two-year period.

Unable to mine keywords included in the full text, so terms only described in the abstract, regardless of whether other phrases are included in the complete article.

## Conclusion

The results of this study suggest that the introduction of lecanemab has not had a significant impact on the research landscape of Alzheimer's disease in the year after the accelerated approval. LDA

# Acknowledgements {.appendix}

I would like to thank my supervisor Emma Rand for her constant support and guidance throughout my project. I would also like to dedicate this project to my late granddad Alan Scrimshire who passed away on the 31st March 2023 after fighting a five year battle with Alzheimer’s Disease. I hope this project highlights the complexity of the disease and the vast efforts being undertaken to find a cure.

# Appendix

```{r}
#| label: fig-topic-model-bigrams
#| include: true
#| fig-cap: "**Bigram LDA Topic Modelling.** Outputs from the post-lecanemab (after 06/01/2023) corpus model with ten topics. Figures show the top ten most commonly associated bigrams in each topic ordered by their per-topic-per-word probability, β. Topic titles were manually created and added."
#| fig-subcap: 
#|  - "Pre-leca"
#|  - "Post-leca"
#| fig-width: 15
#| fig-height: 10
#| fig-align: left

# Pre-leca
top_bigram_terms_pre

top_bigram_terms_post
```

```{r}
#| label: fig-topic-model-trigrams
#| include: true
#| fig-cap: "**Trigram LDA Topic Modelling.** LDA model shows the top ten most commonly associated trigrams in each topic ordered by their per-topic-per-word probability, β, for (A) pre-leca and (B) post-leca corpuses. Topic titles were manually created and added."
#| fig-subcap: 
#|  - "Pre-leca"
#|  - "Post-leca"
#| fig-width: 15
#| fig-height: 10
#| fig-align: left

# Pre-leca
top_trigram_terms_pre

top_trigram_terms_post
```
