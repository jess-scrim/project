---
title: "Impact of novel Alzheimer’s disease drug discovery on the research field using text mining and topic models"
author: "Jess Scrimshire"
date: "`r Sys.Date()`"
engine: knitr
execute:
  echo: false
  include: true
  error: false
  message: false
  warning: false
  cache: true
  freeze: true
bibliography: references.bib
---

```{r}
#| label: packages
#| include: false
library(rmarkdown)
source("scripts/00_setting_up.R")
```

```{r}
#| label: data
#| include: false
load("project.RData")
tidy_abstracts_clean <- read_csv("results/tidy_abstracts_clean.csv")
abstract_trigrams <- read_csv("results/abstract_trigrams.csv")
abstract_bigrams <- read_csv("results/abstract_bigrams.csv")

abstract_na_n <- nrow(filter(abstracts, is.na(title)))
abstract_n <- nrow(filter(abstracts, !is.na(title)))
```

# Introduction

## Alzheimer's Disease

Alzheimer’s disease (AD) is a chronic neurodegenerative disease and the most common cause of dementia. Affecting over 55 million people worldwide, the predominant symptoms of AD usually manifest after the age of 65 and include cognitive impairment, as well as physical and emotional difficulties [@noauthor_2023-ek]. 

The underlying aetiology of sporadic AD, as opposed to genetically caused familial AD, is largely unknown, however research consensus suggests the accumulation of abnormal proteins, amyloid-beta and tau, disrupt cellular pathways and lead to 

AD is progressive and neuronal damage accumulates over a patient’s lifetime leading to hippocampal atrophy. As well as the degeneration of neurons, AD is associated with the build up of abnormal proteins, amyloid-beta and tau [@Villemagne2013-xk]. 

## Treatments for AD

Although there are currently no therapies or interventions that can cure AD, several disease-modifying treatments exist that can alter the course of the disease, alleviate symptoms, and enhance the overall quality of life for patients. A recent comprehensive review by [@Huang2023-vq], identified a shift in clinical trial research, which highlighted more Phase I studies being conducted and more Phase III trials involving anti-amyloid therapies. These trials are involving more patients with early onset AD and mild cognitive impairment (MCI) to help develop preventative therapies. Global estimates for people living with preclinical AD or positive for AD pathology biomarkers were 69 and 315 million, respectively [@Gustavsson2023-qr], therefore increasing research focus on these patient populations is imperative to slow the progression of the disease. 

There have recently been two drugs granted approval by the United States Food and Drug Administration (US FDA) which target the pathophysiologies of AD; aducanumab and lecanemab ([@Center_for_Drug_Evaluation2023-gy]; [@Office_of_the_Commissioner2023-hu]). These treatments are human monoclonal immunotherapies which aim to target and reduce the beta-amyloid protein aggregates in the brain by binding to its various forms in the amyloid-beta pathway. Furthermore, there are four more anti-amyloid monoclonal antibody treatments which have undergone or are currently in Phase III clinical trials [@Cummings2023-lo]. For this study, we will focus on lecanemab, the most recent AD treatment to undergo accelerated approval by the FDA, which was fully approved for treatment in early AD on the 6th July 2023. 

## Topic Modelling and Text Mining in AD Research

Given the consistent publication of thousands of articles every year concerning AD, focusing on early-stage drug discovery could lead to more literature and clinical findings being published. Systematic reviews and meta-analyses are time consuming and labour intensive, and pose a significant challenge to updating the current understandings in the research literature [@Higgins2019-kn]. Topic modelling, a prominent text mining technique, can find patterns and relationships within natural language data, and could provide an automated and unbiased overview of research text. The most common topic modelling method is Latent Dirichlet Allocation (LDA) which assumes, for unstructured text data like research publication, that each document is made up of a number of topics and that each topic is made up of a collection of words [@Blei2003-lh]. Each LDA topic is represented as a probability of words within a topic and a probability of topics within each document, which each follow a Dirichlet distribution.

In silico topic modelling has been used for various applications relating to AD, including describing the research landscape. [@Martinelli2022-ic], identifying novel biomarkers [@Greco2012-pv], and drug repurposing [@Nian2022-xw]. Martinelli performed a nine-topic LDA model and identified five mechanistic themes, as well as a topic relating to AD diagnosis and three concerning treatments. To the best of my knowledge, no studies have explored the change to the AD research landscape with the emergence of newly approved  immunotherapy treatments. To increase the findability and reduce bias when selecting articles, the litsearchr package will help identify the most important terms which we will reference our search terms against [@Grames2019-as].


# Methods

A full summary of the methodology is provided in Appendix 1. All data analysis and visualisations were done in R version 4.3.2 using *tidyverse* packages [@Wickham2019-rj] unless otherwise stated.

## Data Acquisition

Due to accessing constraints, abstracts represent the only document content for this study. Titles, full abstract text, author list, and publication date were obtained from PubMed using the inclusion criteria described in @tbl-inclusion-criteria and *EUTils*, accessed through *Rismed* [@Kovalchik2021-xq]. After excluding publications without abstracts (n = `r abstract_na_n`), `r abstract_n` manuscripts comprised the final dataset (data cut accessed on 10th October 2023). PubMed is a large, online search engine containing over 36 million citations for biomedical literature, and is maintained by the United States National Library of Medicine (NLM) at the National Institutes of Health (NIH). The diversity of entries into the database ensures that the contents are representative and studies are reliable as they are obtained from multiple sources. Entries are assigned Medical Subject Headings (MeSH) which identify health-related terms within each document, therefore classifying articles according to their subject nature which reduces potential interpretive bias. 

```{r}
#| label: tbl-inclusion-criteria
#| include: true
#| tbl-cap: "Inclusion Criteria"
#| tbl-width: 10
#| tbl-height: 3.5
#| tbl-align: left

criteria <- data.frame(x = c("**Criteria:**",
"MeSH term",
"Article Type",
"",
"",
"",
"",
"",
"",
"",
"",
"Publication Date",
"Language"),
y = c("**Filter applied:**",
"‘Alzheimer’s Disease’",
"Books",
"Case Reports",
"Clinical Study",
"Clinical Trial",
"Controlled Clinical Trial",
"Meta-analysis",
"Randomised Controlled Trial",
"Review",
"Systematic Review",
"1st January 2022 to 1st January 2024 inclusive",
"English")
)

# make dataframe filter and criteria

knitr::kable(criteria, col.names = NULL)

```

```{r}
#| label: fig-preprocess
#| include: true
#| fig-cap: "Data Preprocessing"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

knitr::include_graphics("data/preprocess1.png")
knitr::include_graphics("data/preprocess2.png")
```

## Data Preprocessing

Abstracts and their metadata were categorised into “pre-leca” or “post-leca” corpuses based on their publication date relative to the date of lecanemab’s accelerated early approval, 6th January 2023. Full abstract text was tokenised into single words using the unnest_tokens function of the tidytext package [@Silge2016-jf]. The same function was used for tokenising to bigrams and trigrams, using n = 2 and n = 3 respectively. Stop words from the tidytext package [@Silge2016-jf] combined with personalised words (@tbl-appendix2) were then removed @fig-preprocess. To prevent the different spellings of the same word or phrase from being counted multiple times, similar words were mapped to the same variable. For example, ‘*amyloid β*, ‘*beta amyloid*’, and ‘*amyloid aβ*’  were all mapped to ‘*amyloid beta*’.

## Data Analysis

### *litsearchR*

To reassure us that the PubMed search query encapsulated all literature, *litsearchr* package was used to expand the search terms [@Grames2019-as]. Citations from PubMed results using the previous search criteria were read into R. The combined unique keyword and titles, as not all articles have keywords,  for each result were collected. To ensure only the most relevant terms were searched, stop words were removed, as previously described, and the minimum frequency of words was set as n = 50 for keywords and n = 75 for the title. A matrix of each word in each article was created and the potential search terms were ranked with *create_network* and strength [@Barrat2004-hm] from the igraph package [@Csardi2006-gb]. The 80% cutoffs were determined to find only the most important terms to the articles; all others were discarded (@fig-ad-search-terms).

```{r}
#| label: search-terms
#| include: false
#| cache: true

naive_results <- import_results(file="data/pubmed-alzheimerd-set.nbib")

nrow(naive_results)

keywords <- extract_terms(keywords=naive_results[, "keywords"], 
                          method="tagged", 
                          min_n = 1, # allows single words
                          min_freq = 50) # only words that appear at least 10 times in keyword search 

# Remove stop-words from titles
clin_stopwords <- read_lines("data/clin_stopwords.txt")
all_stopwords <- c(get_stopwords("English"), clin_stopwords)

title_terms <- extract_terms(
  text = naive_results[, "title"],
  method = "fakerake",
  min_freq = 75, 
  min_n = 1,
  stopwords = all_stopwords
)

search_terms <- c(keywords, title_terms) %>% unique()


### Network analysis ###

# Combine title with abstract
docs <- paste(naive_results[, "title"], naive_results[, "abstract"])

# Create matrix of which term appears in which article
dfm <- create_dfm(elements = docs, 
                  features = search_terms)

# Create network of linked terms
g <- create_network(dfm, 
                    min_studies = 3)
ggraph(g, layout="stress") +
  coord_fixed() +
  expand_limits(x=c(-3, 3)) +
  geom_edge_link(aes(alpha=weight)) +
  geom_node_point(shape="circle filled", fill="white") +
  geom_node_text(aes(label=name), 
                 hjust="outward", 
                 check_overlap=TRUE) 

## Pruning ##

# Remove terms that are not connected to other terms - strength
strengths <- strength(g)

term_strengths <- data.frame(term=names(strengths), strength=strengths, row.names=NULL) %>%
  mutate(rank = rank(strength, 
                   ties.method="min")) %>%
  arrange(strength)

# Visualise to determine cutoff
cutoff_fig <- ggplot(term_strengths, aes(x=rank, 
                                         y=strength, 
                                         label=term)) +
  geom_line() +
  geom_point() +
  geom_text(data=filter(term_strengths, rank>5), hjust="right", nudge_y=20, check_overlap=TRUE)

cutoff_fig

# Find 80% cutoff
cutoff_cum <- find_cutoff(g, 
                          method="cumulative", 
                          percent=0.8)

# Add to figure
cutoff_fig +
  geom_hline(yintercept = cutoff_cum, 
             linetype = "dashed")

# Add cutoffs for changes
cutoff_change <- find_cutoff(g, 
                             method = "changepoint", 
                             knot_num = 3)

```

```{r}
#| label: fig-ad-search-terms
#| include: true
#| fig-cap: "Search term network for Alzheimer's Disease"
#| fig-width: 10
#| fig-height: 7
#| fig-align: left

cutoff_fig +
  geom_hline(yintercept = cutoff_change, 
             linetype="dashed")

```

```{r}
#| label: drug-search-terms
#| include: false 
#| cache: true

naive_drug_results <- import_results(file="data/pubmed-lecanemabO-set.nbib")

keywords <- extract_terms(keywords=naive_drug_results[, "keywords"], 
                          method="tagged", 
                          min_n = 1, # allows single words
                          min_freq = 2) # only words that appear at least 2 times in keyword search 

# Remove stop-words from titles
clin_stopwords <- read_lines("data/clin_stopwords.txt")
all_stopwords <- c(get_stopwords("English"), clin_stopwords)

title_terms <- extract_terms(
  text = naive_drug_results[, "title"],
  method = "fakerake",
  min_freq = 2, 
  min_n = 1,
  stopwords = all_stopwords
)

# Combine search terms & remove duplicates
search_terms <- c(keywords, title_terms) %>% unique()

## Network analysis ###

# Combine title with abstract
docs <- paste(naive_drug_results[, "title"], naive_drug_results[, "abstract"])

# Create matrix of which term appears in which article
dfm <- create_dfm(elements = docs, 
                  features = search_terms)

# Create network of linked terms
g <- create_network(dfm, 
                    min_studies = 3)
ggraph(g, layout="stress") +
  coord_fixed() +
  expand_limits(x=c(-3, 3)) +
  geom_edge_link(aes(alpha=weight)) +
  geom_node_point(shape="circle filled", fill="white") +
  geom_node_text(aes(label=name), 
                 hjust="outward", 
                 check_overlap=TRUE) 

## Pruning ##

# Remove terms that are not connected to other terms - strength
strengths <- strength(g)

term_strengths <- data.frame(term=names(strengths), strength=strengths, row.names=NULL) %>%
  mutate(rank=rank(strength, ties.method="min")) %>%
  arrange(strength)

# Visualise to determine cutoff
cutoff_fig <- ggplot(term_strengths, aes(x=rank, y=strength, label=term)) +
  geom_line() +
  geom_point() +
  geom_text(data=filter(term_strengths, rank>5), hjust="right", nudge_y=20, check_overlap=TRUE)

# Find 80% cutoff
cutoff_cum <- find_cutoff(g, 
                          method="cumulative", 
                          percent=0.8)

# Add to figure
cutoff_fig +
  geom_hline(yintercept=cutoff_cum, linetype="dashed")

# Add cutoffs for changes
cutoff_change <- find_cutoff(g, method="changepoint", knot_num=3)

```

```{r}
#| label: fig-ad-drug-search-terms
#| include: true
#| fig-cap: "AD drug search term network"
#| fig-width: 10
#| fig-height: 7
#| fig-align: left

cutoff_fig +
  geom_hline(yintercept=cutoff_change, linetype="dashed")

```

```{r}
#| label: data-import
#| include: false
tidy_abstracts_clean <- tidy_abstracts_clean %>%
  anti_join(stop_words)
tidy_abstracts_clean <- tidy_abstracts_clean %>%
  anti_join(my_stopwords)

```

### Publication Frequency

Number of abstracts published per month were visualised as well as the frequency of dates of publications for papers containing the AD drug lecanemab and associated terminology were also obtained.

### Term Frequency

#### N-gram Frequency Analysis

After tokenisation, the top 20 most frequent unigrams were determined for each dataset. The top 20 most frequent bigrams and trigrams were also determined due to many unigrams being associated with pairs or triplets of words. For example, “mild cognitive impairment” relates to a neurological condition, whereas the words “mild”, “cognitive” and “impairment” have ambiguous connotations individually. The most frequent bigrams were also visualised as networks.

#### Term Usage Over Time

The distribution of terms used over 1000 times from the unigram analysis was visualised. Generalised linear model (GLM) was used to determine whether there was a significant change in word usage over the months.

### Topic Modelling

A document term matrix (dtm) was constructed for each dataset, indicating each word’s term frequency (tf), which is a measure of how often a word appears in a document. To determine if a statistical model could distinguish between pre-leca and post-leca corpuses, a two-topic Latent Dirich Allocation (LDA) model [@Blei2003-lh] was applied to the dtm using the topicmodels package [@Grun2011-do]. The per-document-per-topic probabilities (γ) is extracted to show the proportion of words generated in each topic and how often these words appear in either the pre-leca or post-leca corpuses.

Furthermore, two ten-topic LDA models were created, one for each of the pre-leca and post-leca text corpuses, to determine the most frequent topics, where an arbitrary topic number (k) of ten was chosen. The per-topic-per-word probabilities (β) were extracted and the top 10 terms most commonly found in each topic were visualised. As a lot of the most common words appear as bigrams in the text corpuses, the per-topic-per-bigram probabilities with the top 10 most common bigrams were also visualised with two more ten-topic LDA models. In each model the abstracts are considered mixtures of topics and each topic is considered a mixture of words.


# Results

## AD Research Publication

This study found `r nrow(abstracts)` papers that were published between `r min(tidy_abstracts_clean$date)` and `r max(tidy_abstracts_clean$date)` that contained the identified terms.

The distribution of publications is shown in @fig-publication-date.

```{r}
#| label: fig-publication-date
#| include: true
#| fig-cap: "**Publication date distribution of abstracts**: yayady"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

abstracts %>%  
  ggplot(aes(date)) +
  geom_histogram(bins = 100) +
  xlab("Date of Publication") +
  ylab("Number of Abstracts") +
  # 10 month intervals
  scale_x_date(date_breaks = "4 month", date_labels = "%m/%Y") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) 

```

The distribution of publications containing the terms associated with 'lecanemab' is shown in @fig-lecanemab-publication-date.

```{r}
#| label: fig-lecanemab-publication-date
#| include: true
#| fig-cap: "Publication date distribution of abstracts containing lecanemab terms"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

## Visualise abstracts published with all lena terms
naive_drug_results %>% 
  filter(!is.na(date_published)) %>% 
  ggplot(aes(as.Date(date_published, format = "%Y %b %d"))) +
  geom_histogram(bins = 30,
                 binwidth = 100) +
  xlab("Date of Publication") +
  ylab("Number of Abstracts") +
  # 10 month intervals
  scale_x_date(date_breaks = "6 months", date_labels = "%m/%Y") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) 

```

## N-gram Analysis

### Unnigrams

The most frequent unigrams are shown in @fig-tokenisation.

```{r}
#| label: fig-tokenisation
#| include: true
#| fig-cap: "Most Common Unigram"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

tidy_abstracts_clean %>% 
  group_by(type) %>%
  count(word, sort = TRUE) %>%
  slice_head(n = 15) %>% 
  ggplot(aes(n, reorder(word, n), fill = type)) +
  geom_col() +
  labs(y = NULL) +
  xlab("Count") +
  facet_wrap(~type, scale = "free") +
  theme(legend.position = "none",
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        strip.background = element_blank())

# most common word facet by type: pre-leca, post-leca

```

The distribution by month of the top frequent words, as determined from @fig-tokenisation, is shown in @fig-glm-words.

```{r}
#| label: fig-glm-words
#| include: true
#| fig-cap: "Most Frequent Words per Month"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

## Create Generalised linear model

top_words <- c("disease", "brain", "studies", "diseases", "review", "cognitive", "dementia", "neurodegenerative", "clinical", "patients", "treatment", "disorders", "risk", "effects")

# Get word frequency per month
glm_abstracts <- tidy_abstracts_clean %>%  
  filter(word %in% top_words) %>% 
  mutate(date = floor_date(date, "month")) %>% # round date to month
  group_by(date) %>%
  count(word, sort = TRUE) %>%
  ungroup() %>%
  group_by(word) %>%
  mutate(freq = n / sum(n)) %>%
  ungroup() 

# Generalised linear model
glm <- glm( freq ~ date + word, data = glm_abstracts, family = "poisson" )
#summary(glm)

# plot just months of date

glm %>% ggplot(aes(x = date, y = freq)) +
  geom_line(aes(color = word)) +
  ylab("Frequency") +
  xlab("Date") +
  scale_x_date(date_breaks = "6 months", date_labels = "%m/%Y") +
  theme_classic()
```

### Bigrams

```{r}
#| label: bigram-cleaning
#| include: false
bigrams_separated <- abstract_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
bigrams_separated <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word &
         !word1 %in% my_stopwords$word) %>% # remove word1 if stopword
  filter(!word2 %in% stop_words$word &
         !word2 %in% my_stopwords$word) # remove word2 if stopword

# join word1 and word2 back together into bigram
bigrams_united <- bigrams_separated %>%
  unite(bigram, word1, word2, sep = " ")  # 'bigram' name of new column
  
## Map words and remove abbreviations ##
bigrams_united <- bigrams_united %>% 
 # filter(grepl("alzheimer*\\b*disease*", bigram)) %>% 
  mutate(bigram = str_replace_all(bigram, 
                              "\\b(neurodegenerative dis(?:ease|eases|order|orders)?)\\b|\\b(neurological dis(?:ease|eases|order|orders)?)\\b", 
                              "neurodegenerative disease"),
         bigram = str_replace_all(bigram,
                                  "\\b(central nervous)\\b|\\b(system cns)\\b",
                                  "cns"),
         bigram = str_replace_all(bigram,
                                  "\\b(parkinson's disease)\\b|\\b(disease pd)\\b",
                                  "parkinson's disease"),
         bigram = str_replace_all(bigram,
                                  "\\b(blood brain)\\b|\\b(brain barrier)\\b",
                                  "blood brain"),
         bigram = str_replace_all(bigram,
                                  "\\b(amyloid\\s*\\p{Greek})\\b|\\b(amyloid beta)\\b|\\b(amyloid a\\p{Greek})\\b|\\b(beta amyloid)\\b|\\b(\\p{Greek} amyloid)\\b",
                                   "amyloid beta"),
         bigram = str_replace_all(bigram,
                                  "\\b(2019 covid)\\b|\\b(covid 19)\\b",
                                  "covid 19"))

# All abstracts
bigram_counts <- bigrams_united %>% 
  group_by(type) %>% 
  count(bigram, sort = TRUE) %>% 
  ungroup()

# Filter for only relatively common combinations
bigram_graph <- bigrams_united %>%
  count(bigram) %>% 
  filter(n > 200) %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
  
bigram_graph <- bigram_graph %>%
  graph_from_data_frame() # most common word1->word2

set.seed(2017) # set random 2017

# Pre-leca
pre_leca_graph <- bigrams_united %>%
  filter(type == "pre-leca") %>%
  count(bigram) %>% 
  filter(n > 200) %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

pre_leca_graph <- pre_leca_graph %>%
  graph_from_data_frame()

# Post-leca
post_leca_graph <- bigrams_united %>%
  filter(type == "post-leca") %>%
  count(bigram) %>% 
  filter(n > 200) %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
post_leca_graph <- post_leca_graph %>%
  graph_from_data_frame()
```

Exploring some of the bigrams the word "disease" are shown in @tbl-tbl-bigram-plot. The most common bigrams beginning with "neuro" are shown in @tbl-tbl-bigram-plot.

```{r}
#| label: tbl-bigram-plot
#| include: true
#| fig-cap: "Bigrams in abstracts"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

bigrams_separated %>%
  filter(word2 == "disease") %>%
  count(word1, sort = TRUE) %>% 
  slice_head(n = 15) %>%
  knitr::kable(caption = "Most common bigrams ending in 'disease'
")

bigrams_separated %>%
  filter(grepl("^neuro", word1)) %>%
  count(word2, sort = TRUE) %>% 
  slice_head(n = 15) %>%
  knitr::kable(caption = "Most frequent bigrams beginning with 'neuro' in first word
")

bigrams_separated %>%
  filter(grepl("^neuro", word2)) %>%
  count(word1, sort = TRUE) %>% 
  slice_head(n = 15) %>%
  knitr::kable(caption = "Most frequent bigrams beginning with 'neuro' in second word
")
```

Relations of bigram networks are shown in @fig-bigram-visualisation. Bigram networks for pre-leca and post-leca are shown in @fig-bigram-visualisation-pre and @fig-bigram-visualisation-post respectively. Strength networks for bigrams are shown in @fig-bigram-visualisation-strength and split into pre-leca and post-leca in @fig-bigram-visualisation-strength-pre and @fig-bigram-visualisation-strength-post respectively.

```{r}
#| label: fig-bigram-visualisation
#| include: true
#| fig-cap: "Bigram networks"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

# Visualise how word1 relates to word2
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  ggtitle("Bigram Relations")

```

```{r}
#| label: fig-bigram-visualisation-pre
#| include: true
#| fig-cap: "Bigram networks Pre"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left
ggraph(pre_leca_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  ggtitle("Pre-leca Bigram Relations")

```

```{r}
#| label: fig-bigram-visualisation-post
#| include: true
#| fig-cap: "Bigram networks Post"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left
ggraph(post_leca_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  ggtitle("Post-leca Bigram Relations")

```

```{r}
#| label: fig-bigram-visualisation-strength
#| include: true
#| fig-cap: "Bigram networks strength"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left
# Visualise - thickness of line determines how strong the relationship is
set.seed(2020)

bigram_graph %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void() +
  ggtitle("Bigram Relations")

```

```{r}
#| label: fig-bigram-visualisation-strength-pre
#| include: true
#| fig-cap: "Bigram networks Strength Pre"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

# Pre-leca
pre_leca_graph %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void() +
  ggtitle("Pre-leca Bigram Relations")

```

```{r}
#| label: fig-bigram-visualisation-strength-post
#| include: true
#| fig-cap: "Bigram networks Strength Post"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

# Pre-leca
post_leca_graph %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void() +
  ggtitle("Post-leca Bigram Relations")
```

### Trigrams

```{r}
#| label: trigram clean
#| include: false
trigrams_separated <- abstract_trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")
trigrams_separated <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word &
           !word1 %in% my_stopwords$word) %>% # remove word1 if stopword
  filter(!word2 %in% stop_words$word &
           !word2 %in% my_stopwords$word) %>% # remove word2 if stopword
  filter(!word3 %in% stop_words$word &
           !word3 %in% my_stopwords$word) # remove word3 if stopword

# trigram_counts <- trigrams_separated %>% 
#   group_by(type) %>% 
#   count(word1, word2, word3, sort = TRUE) %>% 
#   ungroup()

trigrams_united <- trigrams_separated %>%
  group_by(type) %>% 
  unite(trigram, word1, word2, word3, sep = " ") %>%  # 'trigram' name of new column
  ungroup()

trigrams_united <- trigrams_united %>%
  mutate(trigram = str_replace_all(trigram, 
                                   "\\b(?:mild cognitive impairment|mci|cognitive impairment mci)\\b", 
                                   "mild cognitive impairment"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:central nervous system|nervous system cns|central nervous system)\\b", 
                                   "central nervous system"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:blood brain barrier|brain barrier bbb)\\b", 
                                   "blood-brain barrier"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:amyotrophic lateral sclerosis|lateral sclerosis als|amyotropic lateral sclerosis)\\b",
                                   "amyotrophic lateral sclerosis"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:amyloid beta aβ|β amyloid aβ|amyloid β aβ|amyloid β peptide|amyloid β protein)\\b",
                                   "amyloid beta aβ"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:type 2 diabetes|diabetes mellitus t2dm|2 diabetes mellitus)\\b", 
                                   "type 2 diabetes"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:huntington's disease hd|disease huntinton's disease)\\b",
                                   "huntington's disease hd"),
         trigram = str_replace_all(trigram,
                                   "\\b(?:parkinson's disease pd|disease parkinson's disease)\\b",
                                   "parkinson's disease pd"))

trigram_counts <- trigrams_united %>% 
  group_by(type) %>%
  count(trigram, sort = TRUE) %>%
  ungroup()
```

Most common trigrams in pre- or post-leca text corpuses are shown in @fig-trigram-counts

```{r}
#| label: fig-trigram-counts
#| include: true
#| fig-cap: "Most common trigrams in pre- or post-leca text corpuses"
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: left

trigram_counts %>% 
  group_by(type) %>%
  slice_head(n = 15) %>% 
  ggplot(aes(n, reorder(trigram, n), fill = type)) +
  geom_col() +
  labs(y = NULL) +
  xlab("Count") +
  facet_wrap(~factor(type, levels = c("pre-leca", "post-leca")),
             scale = "free") +
  theme(legend.position = "none",
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        strip.background = element_blank())
```

## Topic Modelling

### Unigrams

```{r}
#| label: LDA Topic Modelling
#| include: false
#| cache: true

word_count_pre <- tidy_abstracts_clean %>%
  filter(type == "pre-leca") %>% 
  count(word, abstract, sort = TRUE) %>% 
  ungroup()

word_count_post <- tidy_abstracts_clean %>%
  filter(type == "post-leca") %>% 
  count(word, abstract, sort = TRUE) %>% 
  ungroup()

# Cast the word counts into a document term matrix
abstract_dtm_pre <- word_count_pre %>%
  cast_dtm(abstract, word, n) 

abstract_dtm_post <- word_count_post %>%
  cast_dtm(abstract, word, n)

# Running the LDA model
abstract_lda_pre <- LDA(abstract_dtm_pre, k = 10, control = list(seed = 1234))

abstract_lda_post <- LDA(abstract_dtm_post, k = 10, control = list(seed = 1234))

tidy_lda_pre <- tidy(abstract_lda_pre,
                     matrix = "beta") %>% 
  mutate(type = "pre_leca")
tidy_lda_post <- tidy(abstract_lda_post,
                      matrix = "beta") %>% 
  mutate(type = "post_leca")

# Pre-leca
top_terms_pre <- tidy_lda_pre %>%
  filter(term != "disease") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Post-leca
top_terms_post <- tidy_lda_post %>%
  filter(term != "disease") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_post <- top_terms_post %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic: Post Leca",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")

top_terms_pre <- top_terms_pre %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic: Pre-Leca",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

The top 10 terms in each pre- and post- leca corpus are shown in @fig-topic-model-pre and @fig-topic-model-post respectively.

```{r}
#| label: fig-topic-model-pre
#| include: true
#| fig-cap: "LDA Topic Modelling"
#| fig-width: 10
#| fig-height: 10
#| fig-align: left

# Visualise - top 10 terms per topic
top_terms_pre

```

```{r}
#| label: fig-topic-model-post
#| include: true
#| fig-cap: "LDA Topic Modelling"
#| fig-width: 10
#| fig-height: 10
#| fig-align: left
top_terms_post
```

### Bigrams

```{r}
#| label: LDA Topic Model Bigrams
#| include: false
#| cache: true

# Cast the bigram counts into a document term matrix
bigram_dtm_pre <- bigrams_separated %>%
  filter(type == "pre-leca") %>% 
  unite(bigram, word1, word2, sep = " ") %>% 
  count(abstract, bigram) %>% 
  cast_dtm(abstract, bigram, n) 

bigram_dtm_post <- bigrams_separated %>%
  filter(type == "post-leca") %>% 
  unite(bigram, word1, word2, sep = " ") %>% 
  count(abstract, bigram) %>% 
  cast_dtm(abstract, bigram, n)

bigram_lda_pre <- LDA(bigram_dtm_pre, k = 10, control = list(seed = 1234))
# A LDA_VEM topic model with 10 topics
bigram_lda_post <- LDA(bigram_dtm_post, k = 10, control = list(seed = 1234))
# A LDA_VEM topic model with 10 topics.

# Interpret the model
tidy_bigram_lda_pre <- tidy(bigram_lda_pre, 
                            matrix = "beta")

tidy_bigram_lda_post <- tidy(bigram_lda_post,
                             matrix = "beta")

# Top 10 terms per topic
#   Not including 'neurodegenerative diseases', parkinson\'s disease' and 'cognitive impairment' as these were common to all bar one topics

top_bigram_terms_pre <- tidy_bigram_lda_pre %>%
  filter(!term %in% c("neurodegenerative diseases", "parkinson\'s disease", "cognitive impairment")) %>%  
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_bigram_terms_pre

top_bigram_terms_post <- tidy_bigram_lda_post %>%
  filter(!term %in% c("neurodegenerative diseases", "parkinson\'s disease", "cognitive impairment")) %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_bigram_terms_post

# Visualise
top_bigram_terms_pre <- top_bigram_terms_pre %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 bigrams in each LDA topic: Pre-leca",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")


top_bigram_terms_post <- top_bigram_terms_post %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 bigrams in each LDA topic: Post-leca",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

The top 10 bigrams in topic models for pre- and post- leca corpuses are shown in @fig-topic-model-bigrams-pre and @fig-topic-model-bigrams-post respectively.

```{r}
#| label: fig-topic-model-bigrams-pre
#| include: true
#| fig-cap: "Bigram Pre-leca LDA Topic Modelling"
#| fig-width: 10
#| fig-height: 10
#| fig-align: left

# Pre-leca
top_bigram_terms_pre
```

```{r}
#| label: fig-topic-model-bigrams-post
#| include: true
#| fig-cap: "Bigram Post-leca LDA Topic Modelling"
#| fig-width: 10
#| fig-height: 10
#| fig-align: left

# Pre-leca
top_bigram_terms_post
```

# Appendix

```{r}
#| label: tbl-appendix2
#| include: true
#| tbl-cap: "Personalised Stop Words"
#| tbl-width: 10
#| tbl-height: 3.5
#| tbl-align: left
my_stopwords %>%
  rename("Stop Word" = word) %>%
    knitr::kable()
```